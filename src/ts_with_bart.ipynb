{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from easse.bleu import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=\"fnlp/bart-base-chinese\")\n",
    "#pipe(lines_orig[0:10], max_length=30, min_length=10) #example of how to run the pipeline on input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"fnlp/bart-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_orig = []\n",
    "with open('../mcts-main/dataset/mcts.dev.orig', encoding=\"utf8\") as f:\n",
    "    lines_orig = f.read().splitlines()\n",
    "\n",
    "lines_ref = []\n",
    "for dataset in range(0,5):\n",
    "    filename = str('../mcts-main/dataset/mcts.dev.simp.'+str(dataset))\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines_ref.append(f.read().splitlines())\n",
    "\n",
    "data_dict = {'orig': lines_orig, 'ref': lines_ref[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5dceb3f8ed403e9c7c69f59524d773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize data\n",
    "def preprocess_data(data):\n",
    "    input_data = [dialogue for dialogue in data['orig']]\n",
    "    targets = [dialogue for dialogue in data['ref']]\n",
    "    inputs = tokenizer(input_data, max_length=max_input, padding='max_length', truncation=True)\n",
    "    targets = tokenizer(targets, max_length=max_target, padding='max_length', truncation=True, text_target='List[str]')\n",
    "    #set labels\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    #return the tokenized data\n",
    "    #input_ids, attention_mask and labels\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = ds.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_bleu(\n",
    "#     sys_sent: str,\n",
    "#     ref_sents: List[str],\n",
    "#     smooth_method: str = \"floor\",\n",
    "#     smooth_value: float = None,\n",
    "#     lowercase: bool = False,\n",
    "#     tokenizer: str = \"13a\",\n",
    "#     effective_order: bool = True,\n",
    "# ):\n",
    "\n",
    "# return bleu_scorer.corpus_score(\n",
    "#         sys_sents,\n",
    "#         refs_sents,\n",
    "#     ).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    bleu_score = sentence_bleu(sys_sent=preds, ref_sents=[labels])\n",
    "    return {\n",
    "        'bleu': bleu_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tempu\\AppData\\Local\\Temp\\ipykernel_15296\\4138469974.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    'simplified_save', #save directory\n",
    "    #eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size= 10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=3,\n",
    "    fp16=False #available only with CUDA\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=tokenized_data,\n",
    "    #eval_dataset=tokenized_data['labels'],\n",
    "    #data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679840a04c134dba80102012078c28d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 102}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7885.0894, 'train_samples_per_second': 0.139, 'train_steps_per_second': 0.007, 'train_loss': 2.42289959942853, 'epoch': 2.86}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=54, training_loss=2.42289959942853, metrics={'train_runtime': 7885.0894, 'train_samples_per_second': 0.139, 'train_steps_per_second': 0.007, 'total_flos': 320721377034240.0, 'train_loss': 2.42289959942853, 'epoch': 2.864864864864865})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、在中华人民共和国领土内及中华人民共和国注册的运输工具上就业或者工作的最低年龄为十六周岁；\n"
     ]
    }
   ],
   "source": [
    "print(lines_orig[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bd362323594a21882ea02929f90600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、在中华人民共和国领土内及中华人民共和国注册的运输工具上就业或者工作的最低年龄为十六周岁；\n",
      "[SEP] [CLS] 一 、 在 中 华 人 民 共 和 国 领 土 内 的 运 输 工 具 [SEP]\n"
     ]
    }
   ],
   "source": [
    "#tokenize the conversation\n",
    "sentence = lines_orig[0]\n",
    "model_inputs = tokenizer(sentence,  max_length=max_input, padding='max_length', truncation=True, return_token_type_ids=False)\n",
    "#make prediction\n",
    "raw_pred, _, _ = trainer.predict([model_inputs])\n",
    "#decode the output\n",
    "print(lines_orig[0])\n",
    "print(tokenizer.decode(raw_pred[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
