{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:40:41,891 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:40:45,971 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-02-21 09:40:46,537 - modelscope - INFO - initiate model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:40:46,537 - modelscope - INFO - initiate model from location C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news.\n",
      "2025-02-21 09:40:46,552 - modelscope - INFO - initialize model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:40:49,110 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "2025-02-21 09:40:49,628 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2025-02-21 09:40:49,630 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2025-02-21 09:40:49,638 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:40:49,648 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:40:49,648 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news'}. trying to build by task and model information.\n",
      "2025-02-21 09:40:49,670 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2025-02-21 09:40:49,678 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:40:49,678 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:40:49,678 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-02-21 09:40:49,721 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:40:49,721 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:40:49,721 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:40:52,626 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:40:58,320 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-02-21 09:40:58,777 - modelscope - INFO - initiate model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:40:58,779 - modelscope - INFO - initiate model from location C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news.\n",
      "2025-02-21 09:40:58,787 - modelscope - INFO - initialize model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:41:01,641 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\modelscope\\utils\\checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
      "2025-02-21 09:41:02,119 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2025-02-21 09:41:02,119 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2025-02-21 09:41:02,135 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:41:02,151 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:41:02,152 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news'}. trying to build by task and model information.\n",
      "2025-02-21 09:41:02,169 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2025-02-21 09:41:02,169 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:41:02,185 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:41:02,185 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-02-21 09:41:02,202 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:41:02,202 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:41:02,202 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import re\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "import utils.LS_pipeline as LS_pipeline\n",
    "ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_raner_named-entity-recognition_chinese-base-news')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/BLCU/top_similar.pickle\", 'rb') as handle:\n",
    "    top_similar = pickle.load(handle)\n",
    "with open(\"../data/BLCU/similarity_dict.pickle\", 'rb') as handle:\n",
    "    similarity_dict = pickle.load(handle)\n",
    "with open(\"../data/HSK/HSK_levels.pickle\", 'rb') as handle:\n",
    "    hsk_dict = pickle.load(handle)\n",
    "\n",
    "blcu = pd.read_csv('../data/BLCU/literature_wordfreq.release_UTF-8.txt', header = None, sep=\"\\t\",)\n",
    "blcu.rename(columns={0:\"character\", 1:\"frequency\"}, inplace=True)\n",
    "blcu.set_index(\"character\", inplace=True)\n",
    "blcu[\"frequency\"] = blcu[\"frequency\"].rank(pct=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner time is: 1.4026978999900166\n",
      "complex_word time is: 0.3842667999997502\n",
      "\t Entering choose func...\n",
      "cand embed time: 0.32935929999803193\n",
      "cos sim time: 0.0006557999877259135\n",
      "cand embed time: 0.4394475000008242\n",
      "cos sim time: 0.0005821000086143613\n",
      "cand embed time: 0.3471447000047192\n",
      "cos sim time: 0.0008817999914754182\n",
      "cand embed time: 0.3630323999968823\n",
      "cos sim time: 0.0010704000014811754\n",
      "cand embed time: 0.3168957000016235\n",
      "cos sim time: 0.0006098000012570992\n",
      "cand embed time: 0.3697461999981897\n",
      "cos sim time: 0.0007464000082109123\n",
      "cand embed time: 0.36935509998875204\n",
      "cos sim time: 0.0006308000010903925\n",
      "cand embed time: 0.3474056999984896\n",
      "cos sim time: 0.0007915000023785979\n",
      "choose time is: 3.5504481000098167\n",
      "\t Entering choose func...\n",
      "cand embed time: 0.33010760000615846\n",
      "cos sim time: 0.0006073999975342304\n",
      "cand embed time: 0.311628199997358\n",
      "cos sim time: 0.000564800007850863\n",
      "cand embed time: 0.3217648999998346\n",
      "cos sim time: 0.000679899996612221\n",
      "cand embed time: 0.3123500000074273\n",
      "cos sim time: 0.0005793000018456951\n",
      "cand embed time: 0.3171730000030948\n",
      "cos sim time: 0.0008151999936671928\n",
      "cand embed time: 0.3299129999941215\n",
      "cos sim time: 0.0006232000014279038\n",
      "cand embed time: 0.3250949000066612\n",
      "cos sim time: 0.0008413999894401059\n",
      "cand embed time: 0.30723439999565016\n",
      "cos sim time: 0.0006581000052392483\n",
      "cand embed time: 0.3065387999959057\n",
      "cos sim time: 0.0006775999936508015\n",
      "choose time is: 3.1866159999917727\n",
      "Original sentence:\n",
      " 十个科技业被选定为中国到2025年应主导的重点领域。\n",
      "Simplified sentence:\n",
      " 十个科技业被选为中国到2025年应主导的重点范畴。\n",
      "NER:\n",
      " ['中国']\n",
      "Complex word:  选定\n",
      "Candidates:  ['选中', '选出', '决定', '指定', '选', '选择', '定夺', '挑选']\n",
      "Complex word:  领域\n",
      "Candidates:  ['区域', '范畴', '行业', '范围', '流域', '场合', '部门', '地方', '单位']\n",
      "pipeline time is: 8.807333099990501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'十个科技业被选为中国到2025年应主导的重点范畴。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"他的见解十分独到，对问题的分析非常精辟。\"\n",
    "sentence = \"十个科技业被选定为中国到2025年应主导的重点领域。\"\n",
    "LS_pipeline.LS_pipeline(sentence, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:48:21,984 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:48:25,068 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-02-21 09:48:25,507 - modelscope - INFO - initiate model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:48:25,509 - modelscope - INFO - initiate model from location C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news.\n",
      "2025-02-21 09:48:25,519 - modelscope - INFO - initialize model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:48:28,507 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\modelscope\\utils\\checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
      "2025-02-21 09:48:29,148 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2025-02-21 09:48:29,149 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2025-02-21 09:48:29,171 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:48:29,172 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:48:29,173 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news'}. trying to build by task and model information.\n",
      "2025-02-21 09:48:29,204 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2025-02-21 09:48:29,213 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:48:29,213 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:48:29,213 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-02-21 09:48:29,260 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:48:29,261 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:48:29,263 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-02-21 09:48:34,484 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:48:37,711 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-02-21 09:48:38,126 - modelscope - INFO - initiate model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:48:38,126 - modelscope - INFO - initiate model from location C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news.\n",
      "2025-02-21 09:48:38,144 - modelscope - INFO - initialize model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-21 09:48:40,795 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\modelscope\\utils\\checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
      "2025-02-21 09:48:41,429 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2025-02-21 09:48:41,430 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2025-02-21 09:48:41,449 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:48:41,449 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:48:41,449 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news'}. trying to build by task and model information.\n",
      "2025-02-21 09:48:41,484 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2025-02-21 09:48:41,496 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:48:41,498 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:48:41,499 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-02-21 09:48:41,534 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-21 09:48:41,534 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-21 09:48:41,539 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pipeline for lexical simplification (LS) task using HSK dictionary lookup and best candidate replacement.\"\"\"\n",
    "\n",
    "###### import packages, NER pipeline\n",
    "import re\n",
    "import jieba\n",
    "import torch\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba.posseg as pseg\n",
    "import pickle\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n",
    "ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_raner_named-entity-recognition_chinese-base-news')\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "###### Load vocab data\n",
    "with open(\"../data/BLCU/similarity_dict_v2.pickle\", 'rb') as handle:\n",
    "    similarity_dict = pickle.load(handle)\n",
    "with open(\"../data/HSK/HSK_levels.pickle\", 'rb') as handle:\n",
    "    hsk_dict = pickle.load(handle)\n",
    "# punc = \",!?！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.-\" # possible punctuation\n",
    "blcu = pd.read_csv('../data/BLCU/literature_wordfreq.release_UTF-8.txt', header = None, sep=\"\\t\",)\n",
    "blcu.rename(columns={0:\"character\", 1:\"frequency\"}, inplace=True)\n",
    "blcu.set_index(\"character\", inplace=True)\n",
    "blcu[\"frequency\"] = blcu[\"frequency\"].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_complex_words(tokens: list, HSK_thresh:int = 5, freq_thresh:float = 0.98):\n",
    "    \"\"\"Finds complex words based on one of two criteria: HSK level, and frequency in BLCU data. Assumes\n",
    "    less frequent words are more difficult or less likely to be known.\"\"\"\n",
    "    complex_HSK = [token for token in tokens if (token in hsk_dict and hsk_dict[token]>HSK_thresh)] # find all tokens above HSK level\n",
    "    simple_HSK = [token for token in tokens if (token in hsk_dict and hsk_dict[token]<=HSK_thresh)] # also find simple tokens\n",
    "    complex_freq = [token for token in tokens if (token in blcu.index and blcu.loc[token].values[0]<freq_thresh)] # find all tokens below freq level\n",
    "    complex_words = list(set(complex_HSK).union(set(complex_freq).difference(set(simple_HSK)))) # combine by union\n",
    "    return complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens = jieba.lcut(sentence)\n",
    "tokens = [token for token in tokens if not re.match(r'^\\W+$', token)] # remove punctuation\n",
    "\n",
    "### NER on each token:\n",
    "ner_output = ner_pipeline(sentence)['output'] # run NER pipeline, generate tokens\n",
    "tokens_ner = list(set([d['span'] for d in ner_output if len(d['span'])>1])) # only collect NER longer than 1 character\n",
    "tokens_no_ner = list(set(tokens) - set(tokens_ner)) # get all tokens that are NOT named entities\n",
    "\n",
    "### get complex words:\n",
    "complex_words = find_complex_words(tokens_no_ner)\n",
    "\n",
    "### get candidates and build new sentences:\n",
    "simple_sentence = sentence\n",
    "candidates = [[word]+similarity_dict[word] for word in complex_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for word in candidates:\n",
    "    sentences += [sentence.replace(word[0], candidate) for candidate in word] # new sentence with word replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['十个科技业被选定为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选中为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选出为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选上为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被决定为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被指定为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选取为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选择为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选用为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被定夺为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被自选为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被挑选为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选派为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点领域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点区域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点地域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点范畴。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点行业。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点类别。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点范围。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点学科。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点流域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点水域。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点场合。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点部门。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点地方。',\n",
       " '十个科技业被选定为中国到2025年应主导的重点单位。']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_new = model.encode(sentences, batch_size=32, convert_to_tensor=True) # create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []\n",
    "tstart = perf_counter()\n",
    "embed_new = model.encode(new_sentences) # create embeddings\n",
    "print(\"cand embed time:\", perf_counter()-tstart)\n",
    "similarity.append(np.float32(model.similarity(embed_orig, embed_new)[0][0])) # calculate similarity\n",
    "best_idx = int(np.argmax(similarity))\n",
    "simple_sentence = sentence.replace(word, candidates[best_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
