{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###### import packages, sentence model, and parse html from site url\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "url = 'https://www.bbc.com/zhongwen/articles/c4gl97d2rzjo/simp'\n",
    "site = requests.get(url)\n",
    "site_soup = BeautifulSoup(site.text, 'html.parser')\n",
    "# print(site_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747, 1)\n"
     ]
    }
   ],
   "source": [
    "###### get text from the html and tokenize\n",
    "site_p = site_soup.find_all('p')[0:100]\n",
    "text = \"\"\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\" # possible punctuation\n",
    "\n",
    "for p in site_p:\n",
    "    text += str(p.get_text()) # append new text to the text string\n",
    "\n",
    "text.translate(str.maketrans('', '', string.punctuation)) # convert str format\n",
    "text_re = re.sub(r\"[%s]+\" %punc, \"\", text) # remove punctuation marks\n",
    "\n",
    "tokens = jieba.lcut(text_re) # tokenize\n",
    "tokens_pd = pd.DataFrame(tokens)\n",
    "tokens_pd.drop_duplicates(inplace=True)\n",
    "tokens_pd.rename({0: \"character\"}, axis=1, inplace=True)\n",
    "print(tokens_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>HSK</th>\n",
       "      <th>definition</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>爱</td>\n",
       "      <td>ài</td>\n",
       "      <td>1</td>\n",
       "      <td>love, like, be fond of, be keen on, cherish, b...</td>\n",
       "      <td>[0.37996447, 0.29299688, 0.55704415, 0.0736245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>爱好</td>\n",
       "      <td>àihào</td>\n",
       "      <td>1</td>\n",
       "      <td>love, like, be fond of, be keen on</td>\n",
       "      <td>[0.4404698, 0.29555953, 0.5669606, -0.0190896,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>爸爸</td>\n",
       "      <td>bàba</td>\n",
       "      <td>1</td>\n",
       "      <td>old man, father, papa, pappa, daddy, pa, beget...</td>\n",
       "      <td>[0.05222196, 0.18916288, 0.2597713, -0.585589,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>白</td>\n",
       "      <td>bái</td>\n",
       "      <td>1</td>\n",
       "      <td>white, clear, pure, plain, wrongly written/mis...</td>\n",
       "      <td>[-0.17480375, 0.23118941, -0.27152503, 0.10098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>八</td>\n",
       "      <td>bā</td>\n",
       "      <td>1</td>\n",
       "      <td>det.: eight</td>\n",
       "      <td>[0.25673103, 0.25089717, 0.5315905, -0.3529175...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character pinyin  HSK                                         definition  \\\n",
       "0         爱     ài    1  love, like, be fond of, be keen on, cherish, b...   \n",
       "1        爱好  àihào    1                 love, like, be fond of, be keen on   \n",
       "2        爸爸   bàba    1  old man, father, papa, pappa, daddy, pa, beget...   \n",
       "3         白    bái    1  white, clear, pure, plain, wrongly written/mis...   \n",
       "4         八     bā    1                                        det.: eight   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.37996447, 0.29299688, 0.55704415, 0.0736245...  \n",
       "1  [0.4404698, 0.29555953, 0.5669606, -0.0190896,...  \n",
       "2  [0.05222196, 0.18916288, 0.2597713, -0.585589,...  \n",
       "3  [-0.17480375, 0.23118941, -0.27152503, 0.10098...  \n",
       "4  [0.25673103, 0.25089717, 0.5315905, -0.3529175...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### load HSK dictionary data and create sentence embeddings of definitions\n",
    "HSK_nums = [('1', 1), ('2', 2), ('3', 3), ('4', 4), ('5', 5), ('6', 6), ('7_9', 7)] # HSK definitions to load files\n",
    "\n",
    "HSK = pd.DataFrame()\n",
    "for hsk in HSK_nums:\n",
    "    hsk_df = pd.read_csv(\"../data/Chinese/HSK/HSK\"+hsk[0]+\".tsv\", sep=\"\\t\", header=None)\n",
    "    hsk_df.drop(0, axis=1, inplace=True)\n",
    "    hsk_df.rename(columns={1: \"character\", 2: \"pinyin\", 3: \"definition\"}, inplace=True)\n",
    "    hsk_df.insert(loc=2, column=\"HSK\", value=[hsk[1]]*hsk_df.shape[0])\n",
    "    hsk_df[\"embedding\"] = hsk_df[\"definition\"].map(sentence_model.encode) # create sentence embedding with model\n",
    "    HSK = pd.concat([HSK, hsk_df])\n",
    "HSK.reset_index(inplace=True, drop=True)\n",
    "HSK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character                                                    图像\n",
       "pinyin                                                  túxiàng\n",
       "HSK                                                           7\n",
       "definition                          picture, graph, icon, image\n",
       "embedding     [-0.13980588, 0.34705573, -0.3828236, -0.55116...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### inner merge HSK with tokens to get dictionary data of tokens in the text\n",
    "tokens_with_embed = pd.merge(left=tokens_pd, right=HSK, how='inner', on=\"character\")\n",
    "tokens_with_embed.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK_levels = {'1': (0, 496), \n",
    "              '2': (497, 1259),\n",
    "              '3': (1260, 2225),\n",
    "              '4': (2226, 3219),\n",
    "              '5': (3220, 4286),\n",
    "              '6': (4287, 5420),\n",
    "              '7': (5421, 11035)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK_level = 3\n",
    "num_hsk = HSK_levels[str(HSK_level)][1]\n",
    "num_tokens = tokens_with_embed.index.size\n",
    "\n",
    "simple_char = []\n",
    "for jj in range(num_tokens):\n",
    "    cos_sim = []\n",
    "    if tokens_with_embed[\"HSK\"][jj]<=HSK_level: # token is already simple enough\n",
    "        simple_char.append(np.nan)\n",
    "    else:\n",
    "        for ii in range(num_hsk):\n",
    "            cos_sim.append(sentence_model.similarity(tokens_with_embed[\"embedding\"][jj], HSK[\"embedding\"][ii])[0][0].float())\n",
    "        simple_char.append(HSK.iloc[int(np.argmax(cos_sim)),0])\n",
    "\n",
    "tokens_with_embed[\"simplified\"] = simple_char"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
