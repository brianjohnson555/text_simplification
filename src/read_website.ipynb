{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import packages, sentence model, and parse html from site url\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba\n",
    "import torch\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "language_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "url = 'https://www.bbc.com/zhongwen/articles/c4gl97d2rzjo/simp'\n",
    "site = requests.get(url)\n",
    "site_soup = BeautifulSoup(site.text, 'html.parser')\n",
    "# print(site_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>POS</th>\n",
       "      <th>pinyin</th>\n",
       "      <th>HSK</th>\n",
       "      <th>definition</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>爱</td>\n",
       "      <td>V</td>\n",
       "      <td>ài</td>\n",
       "      <td>1</td>\n",
       "      <td>love, like, be fond of, be keen on, cherish, b...</td>\n",
       "      <td>[0.37996447, 0.29299688, 0.55704415, 0.0736245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>爱好</td>\n",
       "      <td>V/N</td>\n",
       "      <td>àihào</td>\n",
       "      <td>1</td>\n",
       "      <td>love, like, be fond of, be keen on</td>\n",
       "      <td>[0.4404698, 0.29555953, 0.5669606, -0.0190896,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>爸爸</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bàba</td>\n",
       "      <td>1</td>\n",
       "      <td>old man, father, papa, pappa, daddy, pa, beget...</td>\n",
       "      <td>[0.05222196, 0.18916288, 0.2597713, -0.585589,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>白</td>\n",
       "      <td>Adj</td>\n",
       "      <td>bái</td>\n",
       "      <td>1</td>\n",
       "      <td>white, clear, pure, plain, wrongly written/mis...</td>\n",
       "      <td>[-0.17480375, 0.23118941, -0.27152503, 0.10098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>白</td>\n",
       "      <td>Adv</td>\n",
       "      <td>bái</td>\n",
       "      <td>1</td>\n",
       "      <td>white, clear, pure, plain, wrongly written/mis...</td>\n",
       "      <td>[-0.17480375, 0.23118941, -0.27152503, 0.10098...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character  POS pinyin  HSK  \\\n",
       "0         爱    V     ài    1   \n",
       "1        爱好  V/N  àihào    1   \n",
       "2        爸爸  NaN   bàba    1   \n",
       "3         白  Adj    bái    1   \n",
       "4         白  Adv    bái    1   \n",
       "\n",
       "                                          definition  \\\n",
       "0  love, like, be fond of, be keen on, cherish, b...   \n",
       "1                 love, like, be fond of, be keen on   \n",
       "2  old man, father, papa, pappa, daddy, pa, beget...   \n",
       "3  white, clear, pure, plain, wrongly written/mis...   \n",
       "4  white, clear, pure, plain, wrongly written/mis...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.37996447, 0.29299688, 0.55704415, 0.0736245...  \n",
       "1  [0.4404698, 0.29555953, 0.5669606, -0.0190896,...  \n",
       "2  [0.05222196, 0.18916288, 0.2597713, -0.585589,...  \n",
       "3  [-0.17480375, 0.23118941, -0.27152503, 0.10098...  \n",
       "4  [-0.17480375, 0.23118941, -0.27152503, 0.10098...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HSK = pd.read_pickle(\"../data/Chinese/HSK_full\")\n",
    "HSK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'演讲的视频和文字一度在社交媒体上疯传，但后来却从中国互联网上消失。根据路透社和中国媒体的报道，上周高善文的微信帐号因“违反使用规范”无法关注，外界无法透过微信与他取得联络。东北证券首席经济学家付鹏的微信视频号也被禁止关注。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_p[20].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### get text from the html and tokenize\n",
    "site_p = site_soup.find_all('p')\n",
    "text = \"\"\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\" # possible punctuation\n",
    "\n",
    "for p in site_p:\n",
    "    text += str(p.get_text()) # append new text to the text string\n",
    "\n",
    "text.translate(str.maketrans('', '', string.punctuation)) # convert str format\n",
    "text_re = re.sub(r\"[%s]+\" %punc, \"\", text) # remove punctuation marks\n",
    "\n",
    "tokens = jieba.lcut(text_re) # tokenize\n",
    "tokens_l = a = list(dict.fromkeys(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### get text from the html and tokenize\n",
    "site_s = site_soup.find_all(string=True)\n",
    "text = \"\"\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\" # possible punctuation\n",
    "\n",
    "for s in site_s:\n",
    "    text += str(s.get_text()) # append new text to the text string\n",
    "\n",
    "text.translate(str.maketrans('', '', string.punctuation)) # convert str format\n",
    "text_re = re.sub(r\"[%s]+\" %punc, \"\", text) # remove punctuation marks\n",
    "\n",
    "tokens = jieba.lcut(text_re) # tokenize\n",
    "tokens_l = a = list(dict.fromkeys(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\util.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  a = torch.tensor(a)\n"
     ]
    }
   ],
   "source": [
    "###### Cross compute the top embeddings for each vocab word and add to HSK dataframe\n",
    "similarity_t = sentence_model.similarity(HSK[\"embedding\"], HSK[\"embedding\"])\n",
    "top_choice = torch.flip(np.argsort(similarity_t, axis=1)[:,-21:-1], dims=(1,)).numpy().tolist()\n",
    "top_choice_HSK = [[HSK[\"HSK\"][i] for i in row] for row in top_choice] # convert from index values to HSK values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(tokens, max_HSK):\n",
    "    simplified_tokens = dict()\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            char_idx = np.where(HSK[\"character\"]==token)[0][0]\n",
    "            if HSK[\"HSK\"][char_idx]>max_HSK:\n",
    "                top_idx = top_choice[char_idx][next(x[0] for x in enumerate(top_choice_HSK[char_idx]) if x[1] <= max_HSK)] # iterate through and find index of first element below max_HSK\n",
    "                simplified_tokens[token] = HSK[\"character\"].loc[top_idx]\n",
    "            else:\n",
    "                pass# simplified_tokens[idx] = 0\n",
    "        except:\n",
    "            pass# simplified_tokens[idx] = 1\n",
    "    return simplified_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = simplify(tokens_l, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in site_soup.find_all(string=True):  # Get all text nodes\n",
    "    text = element\n",
    "    for old_word, new_word in replacement_dict.items():\n",
    "        if old_word in text:\n",
    "            text = text.replace(old_word, f'<span style=\"color: red;\">{new_word}</span>')\n",
    "    element.replace_with(BeautifulSoup(text, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.html\", \"w\", encoding = 'utf-8') as file: \n",
    "    # prettify the soup object and convert it into a string \n",
    "    file.write(str(site_soup.prettify())) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
