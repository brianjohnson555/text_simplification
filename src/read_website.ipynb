{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and replace webpage\n",
    "Ongoing notebook to read webpage data, perform LS, generate replacement webpage with LS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import packages, sentence model, and parse html from site url\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba\n",
    "import torch\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "language_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "url = 'https://www.bbc.com/zhongwen/articles/c4gl97d2rzjo/simp'\n",
    "site = requests.get(url)\n",
    "site_soup = BeautifulSoup(site.text, 'html.parser')\n",
    "# print(site_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easse.sari import corpus_sari\n",
    "from easse.report import get_all_scores\n",
    "corpus_sari(orig_sents: List[str],\n",
    "    sys_sents: List[str],\n",
    "    refs_sents: List[List[str]],)\n",
    "get_all_scores(orig_sents: List[str],\n",
    "    sys_sents: List[str],\n",
    "    refs_sents: List[List[str]],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 17:12:47,481 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 17:12:50,144 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-01-28 17:12:50,582 - modelscope - INFO - initiate model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-01-28 17:12:50,584 - modelscope - INFO - initiate model from location C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news.\n",
      "2025-01-28 17:12:50,593 - modelscope - INFO - initialize model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-01-28 17:12:53,151 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\modelscope\\utils\\checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
      "2025-01-28 17:12:54,052 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2025-01-28 17:12:54,052 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2025-01-28 17:12:54,068 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-01-28 17:12:54,068 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-01-28 17:12:54,068 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news'}. trying to build by task and model information.\n",
      "2025-01-28 17:12:54,110 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2025-01-28 17:12:54,125 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-01-28 17:12:54,126 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-01-28 17:12:54,127 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-01-28 17:12:54,158 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-01-28 17:12:54,159 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-01-28 17:12:54,160 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "##### import CNER model and function\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_raner_named-entity-recognition_chinese-base-news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK = pd.read_pickle(\"../data/Chinese/HSK_full\")\n",
    "# HSK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = '中国新冠清零政策2022年底结束后，中国政府数次对经济进行刺激，但有节制，效果也不明显。'\n",
    "punc = \",!?！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.-\" # possible punctuation\n",
    "text.translate(str.maketrans('', '', string.punctuation)) # convert str format\n",
    "text_re = re.sub(r\"[%s]+\" %punc, \"\", text) # remove punctuation marks\n",
    "\n",
    "tokens = jieba.lcut(text_re, cut_all=False) # tokenize\n",
    "tokens_l = list(dict.fromkeys(tokens)) #TODO: try: list(set(tokens))\n",
    "\n",
    "ner_output = ner_pipeline(text)['output'] # add batch to ongoing list\n",
    "tokens_ner = list(set([d['span'] for d in ner_output if len(d['span'])>1]))\n",
    "tokens_no_ner = list(set(tokens_l) - set(tokens_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### get text from the html and tokenize\n",
    "site_s = site_soup.find_all(string=True) # if paragraph, choose ('p') as func arg\n",
    "text = \"\"\n",
    "punc = \",!?！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.-\" # possible punctuation\n",
    "\n",
    "for s in site_s:\n",
    "    text += str(s.get_text()) # append new text to the text string\n",
    "\n",
    "text.translate(str.maketrans('', '', string.punctuation)) # convert str format\n",
    "text_re = re.sub(r\"[%s]+\" %punc, \"\", text) # remove punctuation marks\n",
    "\n",
    "tokens = jieba.lcut(text_re, cut_all=False) # tokenize\n",
    "tokens_l = list(dict.fromkeys(tokens)) #TODO: try: list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=1): # define batching function to run NER pipeline (accepts max 512 characters)\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "ner_output = []\n",
    "for b in batch(text_re, n=510): # batch text for processing in NER pipeline\n",
    "    ner_output += ner_pipeline(b)['output'] # add batch to ongoing list\n",
    "tokens_ner = list(set([d['span'] for d in ner_output if len(d['span'])>1]))\n",
    "# ner_overlap = list(set(tokens_l) & set(tokens_ner)) # find intersection of NER words and jieba tokens\n",
    "tokens_no_ner = list(set(tokens_l) - set(tokens_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_choice = torch.tensor(HSK['top_choice'])\n",
    "top_choice_HSK = torch.tensor(HSK['top_choice_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(tokens, max_HSK):\n",
    "    simplified_tokens = dict()\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            char_idx = np.where(HSK[\"character\"]==token)[0][0]\n",
    "            if HSK[\"HSK\"][char_idx]>max_HSK:\n",
    "                top_idx = top_choice[char_idx][next(x[0] for x in enumerate(top_choice_HSK[char_idx]) if x[1] <= max_HSK)] # iterate through and find index of first element below max_HSK\n",
    "                simplified_tokens[token] = HSK[\"character\"].loc[int(top_idx)]\n",
    "            else:\n",
    "                pass# simplified_tokens[idx] = 0\n",
    "        except:\n",
    "            pass# simplified_tokens[idx] = 1\n",
    "    return simplified_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = simplify(tokens_no_ner, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'政策': '纪律'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国新冠清零政策2022年底结束后，中国政府数次对经济进行刺激，但有节制，效果也不明显。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in site_soup.find_all(string=True):  # Get all text nodes\n",
    "    text = element\n",
    "    ### Highlight NER in blue:\n",
    "    for named_entity in tokens_ner:\n",
    "        text = text.replace(named_entity, f'<span style=\"color: blue;\">{named_entity}</span>')\n",
    "\n",
    "    ### Highlight replaced words in red:\n",
    "    for old_word, new_word in replacement_dict.items():\n",
    "        if old_word in text:\n",
    "            text = text.replace(old_word, f'<span style=\"color: red;\">{new_word}</span>')\n",
    "\n",
    "    ### TODO: Highlight words not in HSK list (but not NER) in other color\n",
    "    newtext = BeautifulSoup(text, 'html.parser')\n",
    "    element.replace_with(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.html\", \"w\", encoding = 'utf-8') as file: \n",
    "    # prettify the soup object and convert it into a string \n",
    "    file.write(str(site_soup)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
