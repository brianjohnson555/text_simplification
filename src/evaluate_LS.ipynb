{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance\n",
    "First, evaluate MCTS data on itself. Could this information be used to generate a decision threshold between BART and LS? Then, compare a few methods results:\n",
    "\n",
    "Metrics: BERTscore, SARI, HSK1-3, mean freq.\n",
    "\n",
    "Base data #1: Pseudo data\n",
    "\n",
    "Base data #2: MCTS\n",
    "\n",
    "Methods: (1) LS only\n",
    "\n",
    "(2) BART only\n",
    "\n",
    "(4) LS after BART\n",
    "\n",
    "(5) LS before BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages, data, and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 16:13:42,996 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 16:13:45,268 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-02-20 16:13:45,780 - modelscope - INFO - initiate model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-20 16:13:45,780 - modelscope - INFO - initiate model from location C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news.\n",
      "2025-02-20 16:13:45,790 - modelscope - INFO - initialize model from C:\\Users\\tempu\\.cache\\modelscope\\hub\\damo\\nlp_raner_named-entity-recognition_chinese-base-news\n",
      "2025-02-20 16:13:48,482 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\modelscope\\utils\\checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
      "2025-02-20 16:13:49,078 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2025-02-20 16:13:49,078 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2025-02-20 16:13:49,097 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-20 16:13:49,097 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-20 16:13:49,097 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news'}. trying to build by task and model information.\n",
      "2025-02-20 16:13:49,115 - modelscope - INFO - cuda is not available, using cpu instead.\n",
      "2025-02-20 16:13:49,131 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-20 16:13:49,131 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-20 16:13:49,131 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2025-02-20 16:13:49,162 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2025-02-20 16:13:49,162 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2025-02-20 16:13:49,162 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\tempu\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_raner_named-entity-recognition_chinese-base-news', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#imports: \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import utils.LS_pipeline as LS\n",
    "import utils.TS_pipeline as TS\n",
    "import pickle\n",
    "from evaluate import load\n",
    "\n",
    "# scoring metrics:\n",
    "sari = load(\"sari\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# vocab data:\n",
    "blcu = pd.read_csv('../data/BLCU/literature_wordfreq.release_UTF-8.txt', header = None, sep=\"\\t\",)\n",
    "blcu.rename(columns={0:\"character\", 1:\"frequency\"}, inplace=True)\n",
    "blcu.set_index(\"character\", inplace=True)\n",
    "blcu[\"frequency\"] = blcu[\"frequency\"].rank(pct=True)\n",
    "blcu = blcu.to_dict()['frequency']\n",
    "with open(\"../data/HSK/HSK_levels.pickle\", 'rb') as handle:\n",
    "    hsk_dict = pickle.load(handle)\n",
    "\n",
    "# parallel sentence data:\n",
    "with open('../data/mcts-pseudo/zh_selected.ori', encoding=\"utf8\") as f:\n",
    "    pseudo_orig = f.readlines()\n",
    "with open('../data/mcts-pseudo/zh_selected.sim', encoding=\"utf8\") as f:\n",
    "    pseudo_ref = f.readlines()\n",
    "with open('../data/mcts/mcts.dev.orig', encoding=\"utf8\") as f:\n",
    "    mcts_orig = f.readlines()\n",
    "mcts_ref = []\n",
    "for dataset in range(0,5):\n",
    "    filename = str('../data/mcts/mcts.dev.simp.'+str(dataset))\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        mcts_ref.append(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build custom metric functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_tokenizer(data: list):\n",
    "    return [\" \".join(jieba.cut(sentence)) for sentence in data] # tokenizes Chinese words with spaces\n",
    "\n",
    "def sentence_metrics(sentence):\n",
    "    tokens = [word for word in jieba.cut(sentence)] # get tokens\n",
    "    ## find portion of words in HSK level 1-3:\n",
    "    levels = [hsk_dict[word] for word in tokens if word in hsk_dict]\n",
    "    if levels:\n",
    "        l13 = (levels.count(1) + levels.count(2) + levels.count(3))/len(tokens)\n",
    "    else:\n",
    "        l13 = 0\n",
    "    ## find frequency of words:\n",
    "    freqs = [np.power(blcu[word], 2) for word in tokens if word in blcu] # get squared frequency\n",
    "    freq = np.mean(freqs) # mean of squared freqs\n",
    "\n",
    "    return l13, freq\n",
    "\n",
    "def corpus_metrics(complex_sentences: list, simple_sentences: list):\n",
    "    simple_metrics = [sentence_metrics(sentence) for sentence in simple_sentences]\n",
    "    complex_metrics = [sentence_metrics(sentence) for sentence in complex_sentences]\n",
    "    l13_simple = np.mean([simple_metrics[idx][0] for idx in range(len(simple_metrics))])\n",
    "    l13_complex = np.mean([complex_metrics[idx][0] for idx in range(len(complex_metrics))])\n",
    "    freq_simple = np.mean([simple_metrics[idx][1] for idx in range(len(simple_metrics))])\n",
    "    freq_complex = np.mean([complex_metrics[idx][1] for idx in range(len(complex_metrics))])\n",
    "    l13_score = 100*(l13_simple - l13_complex)/l13_complex # percent change in L1-3 proportion\n",
    "    freq_score = 100*(freq_simple - freq_complex)/freq_complex # percent change in squared frequency\n",
    "    return l13_score, freq_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pipelines to generate simple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BART...\n",
      "Running BARTLS...\n",
      "Running LSBART...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running LS...\")\n",
    "simple_LS = [LS.LS_pipeline(sentence) for sentence in mcts_orig]\n",
    "print(\"Running BART...\")\n",
    "simple_BART = [TS.TS_with_BART(sentence) for sentence in mcts_orig]\n",
    "print(\"Running BARTLS...\")\n",
    "simple_BARTLS = [TS.TS_with_BART_LS(sentence) for sentence in mcts_orig]\n",
    "print(\"Running LSBART...\")\n",
    "simple_LSBART = [TS.TS_with_LS_BART(sentence) for sentence in mcts_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BART...\n",
      "Running BARTLS...\n",
      "Running LSBART...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running LS...\")\n",
    "simple_LS_ps = [LS.LS_pipeline(sentence) for sentence in pseudo_orig[500000:500005]]\n",
    "print(\"Running BART...\")\n",
    "simple_BART_ps = [TS.TS_with_BART(sentence) for sentence in pseudo_orig[500000:500005]]\n",
    "print(\"Running BARTLS...\")\n",
    "simple_BARTLS_ps = [TS.TS_with_BART_LS(sentence) for sentence in pseudo_orig[500000:500005]]\n",
    "print(\"Running LSBART...\")\n",
    "simple_LSBART_ps = [TS.TS_with_LS_BART(sentence) for sentence in pseudo_orig[500000:500005]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/results/simple_LS.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(simple_LS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"../data/results/simple_BART.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(simple_BART, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"../data/results/simple_BARTLS.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(simple_BARTLS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"../data/results/simple_LSBART.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(simple_LSBART, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_MCTS(simple_sentences: list):\n",
    "    return get_metrics(simple_sentences, mcts_orig, mcts_ref)\n",
    "\n",
    "def get_metrics_pseudo(simple_sentences: list):\n",
    "    return get_metrics(simple_sentences, pseudo_orig[500000:500367], [pseudo_ref[500000:500367]])\n",
    "\n",
    "def get_metrics(simple_sentences: list, complex_sentences: list, reference_sentences: list,):\n",
    "    # tokenize sentences:\n",
    "    tokenized_simplified = chinese_tokenizer(simple_sentences)\n",
    "    tokenized_complex = chinese_tokenizer(complex_sentences)\n",
    "    tokenized_reference = [chinese_tokenizer([reference_sentences[idx][ref] \n",
    "                                              for idx in range(len(reference_sentences))]) \n",
    "                                              for ref in range(len(complex_sentences))]\n",
    "\n",
    "    # Compute SARI score:\n",
    "    sari_score = sari.compute(\n",
    "        predictions=tokenized_simplified, # model output\n",
    "        references=tokenized_reference, # reference simple sentences\n",
    "        sources=tokenized_complex # complex sentence\n",
    "    )[\"sari\"]\n",
    "\n",
    "    # Compute BERT precision score:\n",
    "    bert_score = bertscore.compute(\n",
    "        predictions=tokenized_simplified, \n",
    "        references=tokenized_reference, \n",
    "        lang=\"zh\"\n",
    "        )[\"precision\"][0]\n",
    "\n",
    "    # Compute L1-3 and frequency scores:\n",
    "    l13_score, freq_score = corpus_metrics(tokenized_complex, tokenized_simplified)\n",
    "\n",
    "    # Print the result\n",
    "    print(\"SARI Score:\", sari_score)\n",
    "    print(\"BERTScore (precision):\", bert_score)\n",
    "    print(\"L1-3 increase (%):\", l13_score)\n",
    "    print(\"Freq^2 increase (%):\", freq_score)\n",
    "    return {'sari_score': sari_score, \n",
    "            'bert_score': bert_score, \n",
    "            'l13_score': l13_score, \n",
    "            'freq_score': freq_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARI Score: 55.75995861201505\n",
      "BERTScore (precision): 1.0\n",
      "L1-3 increase (%): 17.798284099434706\n",
      "Freq^2 increase (%): 1.1999394822726044\n"
     ]
    }
   ],
   "source": [
    "metric_baseline = get_metrics_MCTS(mcts_ref[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARI Score: 39.9217829139927\n",
      "BERTScore (precision): 0.9346213936805725\n",
      "L1-3 increase (%): 10.865239085432693\n",
      "Freq^2 increase (%): 0.8463001133132716\n",
      "SARI Score: 34.9127452771879\n",
      "BERTScore (precision): 0.9389698505401611\n",
      "L1-3 increase (%): -11.76362961373075\n",
      "Freq^2 increase (%): -0.3636613371313572\n",
      "SARI Score: 32.125925726873795\n",
      "BERTScore (precision): 0.8861942291259766\n",
      "L1-3 increase (%): -3.0725864731664676\n",
      "Freq^2 increase (%): 0.5336767962468105\n",
      "SARI Score: 32.68088788306736\n",
      "BERTScore (precision): 0.8818410634994507\n",
      "L1-3 increase (%): -8.14344688493532\n",
      "Freq^2 increase (%): 0.1443400634633418\n"
     ]
    }
   ],
   "source": [
    "metric_LS = get_metrics_MCTS(simple_LS)\n",
    "metric_BART = get_metrics_MCTS(simple_BART)\n",
    "metric_BARTLS = get_metrics_MCTS(simple_BARTLS)\n",
    "metric_LSBART = get_metrics_MCTS(simple_LSBART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARI Score: 99.38011695906432\n",
      "BERTScore (precision): 1.0000001192092896\n",
      "L1-3 increase (%): 4.614116383851967\n",
      "Freq^2 increase (%): 0.9541224335453964\n"
     ]
    }
   ],
   "source": [
    "metric_baseline_ps = get_metrics_pseudo(pseudo_ref[500000:500005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARI Score: 43.067211146786306\n",
      "BERTScore (precision): 0.8094401955604553\n",
      "L1-3 increase (%): 7.853118829895267\n",
      "Freq^2 increase (%): -0.06386966014961797\n",
      "SARI Score: 44.928491625673836\n",
      "BERTScore (precision): 0.785178542137146\n",
      "L1-3 increase (%): -24.088159449222445\n",
      "Freq^2 increase (%): -1.5699054315998624\n",
      "SARI Score: 41.2368886873214\n",
      "BERTScore (precision): 0.7782107591629028\n",
      "L1-3 increase (%): -14.433226009213602\n",
      "Freq^2 increase (%): -2.168998479340594\n",
      "SARI Score: 44.24405675106732\n",
      "BERTScore (precision): 0.8079299926757812\n",
      "L1-3 increase (%): -13.998877648201296\n",
      "Freq^2 increase (%): -5.085667718351244\n"
     ]
    }
   ],
   "source": [
    "metric_LS_ps = get_metrics_pseudo(simple_LS_ps)\n",
    "metric_BART_ps = get_metrics_pseudo(simple_BART_ps)\n",
    "metric_BARTLS_ps = get_metrics_pseudo(simple_BARTLS_ps)\n",
    "metric_LSBART_ps = get_metrics_pseudo(simple_LSBART_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(columns=[\"Method\", \"SARI\", \"BERTscore\", \"L1-3 (%)\", \"Mean freq rank\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
